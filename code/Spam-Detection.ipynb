{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a script that trains a Multinomial Naive Bayes model to detect spam mails. The script can be executed as follows:\n",
    "\n",
    "To train a model run\n",
    "\n",
    "```\n",
    "script.py train path/to/spam path/to/nospam\n",
    "```\n",
    "\n",
    "To evaluate the model using cross-validation (using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)) run\n",
    "\n",
    "```\n",
    "script.py cross path/to/spam path/to/nospam\n",
    "```\n",
    "\n",
    "To find the best set of parameters using a grid search (using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) run\n",
    "\n",
    "```\n",
    "script.py grid path/to/spam path/to/nospam\n",
    "```\n",
    "\n",
    "The grid search does a limited sweap over the vocabulary size (no limit, 1000000) and the percentage used for detecting stop words (1.0, 0.99, 0.98, 0.95). The latter parameter drops a word from the vocabulary if it occures in more than x percent of the documents.\n",
    "\n",
    "\n",
    "# Spam Detection\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Classifying e-mails into spam and no-spam is not an easy task because the distribution of labels is very inbalanced. Often well over 95% of the labels are spam mails. This makes training a classifier diffictult because the naive prediction (always predicting spam) already achieves 95% accuracy. Of course, this solution is useless as all relevant mails will be filtered out as well.\n",
    "\n",
    "There are several strategies to overcome this problem:\n",
    "\n",
    "* oversampling the minority class to get a 50/50 distribution of labels\n",
    "* undersampling the majority class to get a 50/50 distribution of labels\n",
    "* generating new samples in the minority class that are close to the existing samples in the feature space\n",
    "\n",
    "In this script I didn't employ any of these techniques as the dataset was already balanced.\n",
    "\n",
    "## Simple NLP Approach\n",
    "\n",
    "Many features from the mails can be used to classify spam. For example one can use meta information from the mail header like time, IP address, sender and so forth. I'm sure suffisticated spam filter systems use this kind of information, but modern spam filters all rely heavily on natural language processing to use the mail body for classification. This is the approach that is demonstrated here.\n",
    "\n",
    "### Bag of Words\n",
    "In this script I use the approcha of modelling the natural language as a bag-of-words. That is for every mail we have a long vector that is the length of the vocabulary were each entry represents the number of times that word occures in the mail. In scikiti-learn this can be done with the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Additionally one approach that is often used in document classification and retrieval is to use the so called TF-IDF statistic. It stands for term [term frequencyâ€“inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The term frequency of a term of a specific document is weighted by teh inverse document frequency of that term over the whole corpus.\n",
    "\n",
    "The  The idea is that some words have a high frequency in a document but also occure in a lot of documents in the corpus. They say less about the specific document compared to a word that is frequent in the document but doesn't occure in a lot of documents.\n",
    "\n",
    "Here I use the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) from scikit-learn to generate the TF-IDF feature out of the word count matrix (word count for each word in the vocabulary for all documents).\n",
    "\n",
    "### Naive Bayes\n",
    "For classification I use a Naive Bayes based on the multinomial distribution which models the probability of counts. Perfect! Specifically, I use the [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# TODO: SVM Model\n",
    "# TODO: Try Binary feature\n",
    "# TODO: subject line feature\n",
    "\n",
    "# TODO: Grid search for Binary vs TF vs TF*IDF\n",
    "\n",
    "\n",
    "def load_from_folders(dirs):\n",
    "    print(dirs)\n",
    "    data = DataFrame({'body': [], 'label': []})\n",
    "    for path, label in dirs:\n",
    "        mails, fnames = [], []\n",
    "        for root_dir, dir_names, file_names in os.walk(path):\n",
    "            # load data in sub-directories\n",
    "            for directory in dir_names:\n",
    "                data = data.append(load_from_folders([(os.path.join('.',directory),label)]))\n",
    "            # load files in root directory\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(root_dir, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == '\\n':\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = '\\n'.join(lines)\n",
    "                    mails.append({'body': content, 'label': label})\n",
    "                    fnames.append(file_path)\n",
    "        data = data.append(DataFrame(mails, index=fnames))\n",
    "    return data\n",
    "\n",
    "def create_dataframe(dirs):\n",
    "    data = load_from_folders(dirs)\n",
    "    #data.reset_index().drop_duplicates(subset='index').set_index('index')\n",
    "    return data.reindex(numpy.random.permutation(data.index))\n",
    "\n",
    "def write_prediction(prediction, file_name):\n",
    "    f = open(file_name, \"w\", encoding=\"latin-1\")\n",
    "\n",
    "    for line in zip(prediction.index.values, prediction['label']):\n",
    "        f.write('{0}\\t{1}\\n'.format(line[0].split('/')[-1], line[1]))\n",
    "        #print('{0}\\t{1}'.format(line[0], line[1]))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def create_model():\n",
    "    # HashingVectorizer ?\n",
    "    return Pipeline([\n",
    "            ('count_vectorizer', CountVectorizer(\n",
    "                ngram_range=(1, 2), \n",
    "                strip_accents='unicode', \n",
    "                min_df=2, \n",
    "                max_df=0.90, \n",
    "                stop_words=None, \n",
    "                max_features=1000000,\n",
    "                binary=True)),\n",
    "            # ('idf_transformer', TfidfTransformer(\n",
    "            # norm='l2', \n",
    "            # use_idf=True, \n",
    "            # smooth_idf=True, \n",
    "            # sublinear_tf=False)),\n",
    "            ('classifier', MultinomialNB(\n",
    "                alpha=0.001, \n",
    "                fit_prior=True, \n",
    "                class_prior=None))\n",
    "        ])\n",
    "\n",
    "def load_model(model_name):\n",
    "    with open(model_name, 'rb') as f:\n",
    "        model_attributes = pickle.load(f)\n",
    "        pipeline = create_model()\n",
    "        pipeline.named_steps['count_vectorizer'].vocabulary_ = model_attributes[0]\n",
    "        pipeline.named_steps['count_vectorizer'].stop_words_ = None\n",
    "\n",
    "        pipeline.named_steps['classifier'].class_count_ = model_attributes[1]\n",
    "        pipeline.named_steps['classifier'].feature_count_ = model_attributes[2]\n",
    "        pipeline.named_steps['classifier'].class_log_prior_ = numpy.log(numpy.divide(\n",
    "            model_attributes[1],\n",
    "            numpy.sum(model_attributes[1])\n",
    "        ))\n",
    "        pipeline.named_steps['classifier'].feature_log_prob_ = numpy.transpose(numpy.log(numpy.multiply(\n",
    "            numpy.transpose(pipeline.named_steps['classifier'].feature_count_),\n",
    "            numpy.divide(1.0, pipeline.named_steps['classifier'].class_count_)\n",
    "        )))\n",
    "        pipeline.named_steps['classifier'].classes_ = model_attributes[3]\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "def save_pipeline(pipeline, model_name):\n",
    "    with open(model_name, 'wb') as f:\n",
    "        pickle.dump([\n",
    "            pipeline.named_steps['count_vectorizer'].vocabulary_, \n",
    "            pipeline.named_steps['classifier'].class_count_, \n",
    "            pipeline.named_steps['classifier'].feature_count_,\n",
    "            pipeline.named_steps['classifier'].classes_], f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arguments = sys.argv\n",
    "\n",
    "    if len(sys.argv) == 5:\n",
    "        if arguments[1] == 'classify':\n",
    "            # load model\n",
    "            pipeline = load_model(arguments[2])\n",
    "\n",
    "            # load mails from directory\n",
    "            data = create_dataframe([(arguments[3], '')])\n",
    "\n",
    "            # predict class labels\n",
    "            data['label'] = pipeline.predict(data['body'])\n",
    "\n",
    "            # output the result\n",
    "            print('\\nTotal emails classified:', len(data), \n",
    "                '\\nvocab size:', len(pipeline.named_steps['count_vectorizer'].vocabulary_))\n",
    "\n",
    "            write_prediction(data, arguments[4])\n",
    "\n",
    "        elif arguments[1] == 'learn':\n",
    "            # load training data\n",
    "            data = create_dataframe([(arguments[2], 'SPAM'), (arguments[3], 'NOSPAM')])\n",
    "\n",
    "            # create pipeline\n",
    "            pipeline = create_model()\n",
    "\n",
    "            # train classifier\n",
    "            pipeline.fit(data['body'].values, data['label'].values.astype(str))\n",
    "\n",
    "            # save the model\n",
    "            save_pipeline(pipeline, arguments[4])\n",
    "\n",
    "            with open('backup.model', 'wb') as f:\n",
    "                pickle.dump(pipeline, f)\n",
    "\n",
    "        elif arguments[1] == 'cross':\n",
    "            # load training data\n",
    "            data = create_dataframe([(arguments[2], 'SPAM'), (arguments[3], 'NOSPAM')])\n",
    "\n",
    "            # create pipeline\n",
    "            pipeline = create_model()\n",
    "\n",
    "            # perform 10-fold crossvalidation\n",
    "            scores = cross_val_score(pipeline, data['body'].values, data['label'].values.astype(str), cv=10, n_jobs=2, pre_dispatch=3)\n",
    "\n",
    "            # train classifier\n",
    "            pipeline.fit(data['body'].values, data['label'].values.astype(str))\n",
    "\n",
    "            # output the result\n",
    "            print('Total emails classified:', len(data), '\\nvocab size:', len(pipeline.named_steps['count_vectorizer'].vocabulary_), \n",
    "                '\\nstop words:', len(pipeline.named_steps['count_vectorizer'].stop_words_), '\\n\\n10-Fold-Cross-Validation:')\n",
    "            for index, kscore in enumerate(scores):\n",
    "                print('{0:3}: {1:.3f}  '.format(index+1, kscore), end='\\n')\n",
    "            print('----------\\nAvg: {0:.3f}'.format(sum(scores)/len(scores)))\n",
    "\n",
    "            # save the model\n",
    "            save_pipeline(pipeline, arguments[4])\n",
    "\n",
    "        elif arguments[1] == 'grid':\n",
    "            # load training data\n",
    "            data = create_dataframe([(arguments[2], 'SPAM'), (arguments[3], 'NOSPAM')])\n",
    "\n",
    "            # create pipeline\n",
    "            pipeline = create_model()\n",
    "\n",
    "            # define parameters for grid\n",
    "            param_grid = dict(count_vectorizer__max_features=[None, 1000000],\n",
    "                              count_vectorizer__max_df=[1.0, 0.99, 0.98, 0.95])\n",
    "            grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=0, cv=5, n_jobs=2, pre_dispatch=2)\n",
    "\n",
    "            # start grid search\n",
    "            grid_search.fit(data['body'].values, data['label'].values.astype(str))\n",
    "\n",
    "            # print result\n",
    "            print('Cross-Validation result:\\n', grid_search.grid_scores_)\n",
    "            print('\\nbest\\n', grid_search.best_params_, '\\nscore:', grid_search.best_score_)\n",
    "\n",
    "            # save the best model\n",
    "            save_pipeline(pipeline, arguments[4])\n",
    "\n",
    "        else:\n",
    "            print('Mode', arguments[1], 'not known.')\n",
    "    else:\n",
    "        print('Wrong number of command line arguments!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
