{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the training and test set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# specify the separator and how to handle missing values\n",
    "train_set = pd.read_csv('train.csv', sep=',', keep_default_na=True, na_values=[''], engine='c')\n",
    "test_set = pd.read_csv('test.csv', sep=',', keep_default_na=True, na_values=[''], engine='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved the IDs of the test set customers so that I can directly\n",
    "access them later, however the test set customers also have missing\n",
    "values for the target variable ('churn'). After I saved the test IDs\n",
    "I can set the 'Customer_ID' variable for both parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the IDs of the customers from the test set for later\n",
    "test_Customer_IDs = test_set['Customer_ID']\n",
    "\n",
    "# make the Customer_ID column the index\n",
    "train_set.set_index('Customer_ID', inplace=True)\n",
    "test_set.set_index('Customer_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I concatenate the training and test set into on DataFrame. This makes\n",
    "preprocessing easy every step is automatically applied to both parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the training and test set for easier and\n",
    "# coherent preprocessing.\n",
    "data = pd.concat([train_set, test_set], sort=True)\n",
    "\n",
    "del train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas won't print all columns of the DataFrame since it has\n",
    "172 columns. Hence, I just print them here one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: HHstatin\n",
      "  2: REF_QTY\n",
      "  3: actvsubs\n",
      "  4: adjmou\n",
      "  5: adjqty\n",
      "  6: adjrev\n",
      "  7: adults\n",
      "  8: age1\n",
      "  9: age2\n",
      " 10: area\n",
      " 11: asl_flag\n",
      " 12: attempt_Mean\n",
      " 13: attempt_Range\n",
      " 14: avg3mou\n",
      " 15: avg3qty\n",
      " 16: avg3rev\n",
      " 17: avg6mou\n",
      " 18: avg6qty\n",
      " 19: avg6rev\n",
      " 20: avgmou\n",
      " 21: avgqty\n",
      " 22: avgrev\n",
      " 23: blck_dat_Mean\n",
      " 24: blck_dat_Range\n",
      " 25: blck_vce_Mean\n",
      " 26: blck_vce_Range\n",
      " 27: callfwdv_Mean\n",
      " 28: callfwdv_Range\n",
      " 29: callwait_Mean\n",
      " 30: callwait_Range\n",
      " 31: car_buy\n",
      " 32: cartype\n",
      " 33: cc_mou_Mean\n",
      " 34: cc_mou_Range\n",
      " 35: ccrndmou_Mean\n",
      " 36: ccrndmou_Range\n",
      " 37: change_mou\n",
      " 38: change_rev\n",
      " 39: children\n",
      " 40: churn\n",
      " 41: comp_dat_Mean\n",
      " 42: comp_dat_Range\n",
      " 43: comp_vce_Mean\n",
      " 44: comp_vce_Range\n",
      " 45: complete_Mean\n",
      " 46: complete_Range\n",
      " 47: crclscod\n",
      " 48: creditcd\n",
      " 49: crtcount\n",
      " 50: csa\n",
      " 51: custcare_Mean\n",
      " 52: custcare_Range\n",
      " 53: da_Mean\n",
      " 54: da_Range\n",
      " 55: datovr_Mean\n",
      " 56: datovr_Range\n",
      " 57: div_type\n",
      " 58: drop_blk_Mean\n",
      " 59: drop_blk_Range\n",
      " 60: drop_dat_Mean\n",
      " 61: drop_dat_Range\n",
      " 62: drop_vce_Mean\n",
      " 63: drop_vce_Range\n",
      " 64: dualband\n",
      " 65: dwllsize\n",
      " 66: dwlltype\n",
      " 67: educ1\n",
      " 68: eqpdays\n",
      " 69: ethnic\n",
      " 70: forgntvl\n",
      " 71: hnd_price\n",
      " 72: hnd_webcap\n",
      " 73: income\n",
      " 74: infobase\n",
      " 75: inonemin_Mean\n",
      " 76: inonemin_Range\n",
      " 77: iwylis_vce_Mean\n",
      " 78: iwylis_vce_Range\n",
      " 79: kid0_2\n",
      " 80: kid11_15\n",
      " 81: kid16_17\n",
      " 82: kid3_5\n",
      " 83: kid6_10\n",
      " 84: last_swap\n",
      " 85: lor\n",
      " 86: mailflag\n",
      " 87: mailordr\n",
      " 88: mailresp\n",
      " 89: marital\n",
      " 90: models\n",
      " 91: months\n",
      " 92: mou_Mean\n",
      " 93: mou_Range\n",
      " 94: mou_cdat_Mean\n",
      " 95: mou_cdat_Range\n",
      " 96: mou_cvce_Mean\n",
      " 97: mou_cvce_Range\n",
      " 98: mou_opkd_Mean\n",
      " 99: mou_opkd_Range\n",
      "100: mou_opkv_Mean\n",
      "101: mou_opkv_Range\n",
      "102: mou_pead_Mean\n",
      "103: mou_pead_Range\n",
      "104: mou_peav_Mean\n",
      "105: mou_peav_Range\n",
      "106: mou_rvce_Mean\n",
      "107: mou_rvce_Range\n",
      "108: mouiwylisv_Mean\n",
      "109: mouiwylisv_Range\n",
      "110: mouowylisv_Mean\n",
      "111: mouowylisv_Range\n",
      "112: mtrcycle\n",
      "113: new_cell\n",
      "114: numbcars\n",
      "115: occu1\n",
      "116: opk_dat_Mean\n",
      "117: opk_dat_Range\n",
      "118: opk_vce_Mean\n",
      "119: opk_vce_Range\n",
      "120: ovrmou_Mean\n",
      "121: ovrmou_Range\n",
      "122: ovrrev_Mean\n",
      "123: ovrrev_Range\n",
      "124: ownrent\n",
      "125: owylis_vce_Mean\n",
      "126: owylis_vce_Range\n",
      "127: pcowner\n",
      "128: peak_dat_Mean\n",
      "129: peak_dat_Range\n",
      "130: peak_vce_Mean\n",
      "131: peak_vce_Range\n",
      "132: phones\n",
      "133: plcd_dat_Mean\n",
      "134: plcd_dat_Range\n",
      "135: plcd_vce_Mean\n",
      "136: plcd_vce_Range\n",
      "137: pre_hnd_price\n",
      "138: prizm_social_one\n",
      "139: proptype\n",
      "140: recv_sms_Mean\n",
      "141: recv_sms_Range\n",
      "142: recv_vce_Mean\n",
      "143: recv_vce_Range\n",
      "144: refurb_new\n",
      "145: retdays\n",
      "146: rev_Mean\n",
      "147: rev_Range\n",
      "148: rmcalls\n",
      "149: rmmou\n",
      "150: rmrev\n",
      "151: roam_Mean\n",
      "152: roam_Range\n",
      "153: rv\n",
      "154: solflag\n",
      "155: threeway_Mean\n",
      "156: threeway_Range\n",
      "157: tot_acpt\n",
      "158: tot_ret\n",
      "159: totcalls\n",
      "160: totmou\n",
      "161: totmrc_Mean\n",
      "162: totmrc_Range\n",
      "163: totrev\n",
      "164: truck\n",
      "165: unan_dat_Mean\n",
      "166: unan_dat_Range\n",
      "167: unan_vce_Mean\n",
      "168: unan_vce_Range\n",
      "169: uniqsubs\n",
      "170: vceovr_Mean\n",
      "171: vceovr_Range\n",
      "172: wrkwoman\n"
     ]
    }
   ],
   "source": [
    "for col_i, col in enumerate(data.columns):\n",
    "    print(f\"{col_i+1:>3}: {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "I define a few lists of variables for selective pre-processing for those\n",
    "variables.\n",
    "\n",
    "* `preprocessing_drop` simply drops the variable.\n",
    "* `categorical_ordered` and `categorical_binary` are label encoded\n",
    "* `categorical_unordered` are label encoded and then dummy encoded (aka. one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that are dropped\n",
    "preprocessing_drop = ['area']  # redundant with 'csa'\n",
    "\n",
    "# variables that are label encoded (\"foo\" to 0, \"bar\" to 1, etc.)\n",
    "# and then dummy encoded (one-hot encoding)\n",
    "categorical_unordered = [\n",
    "    'csa', 'prizm_social_one', 'div_type', 'dualband', 'hnd_webcap',\n",
    "    'occu1', 'marital', 'cartype', 'HHstatin', 'proptype', 'ethnic',\n",
    "    'ownrent', 'dwlltype', 'new_cell', 'refurb_new', 'children',\n",
    "    'infobase', 'mailflag', 'solflag', 'kid0_2', 'kid3_5', 'kid6_10',\n",
    "    'kid11_15', 'kid16_17','creditcd','car_buy']\n",
    "\n",
    "# variables that are label encoded\n",
    "categorical_ordered = ['dwllsize','crclscod']\n",
    "categorical_binary = ['mailordr','asl_flag','wrkwoman','mailresp','pcowner']\n",
    "\n",
    "preprocessing_date = ['last_swap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next steps I do all of the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data.drop(preprocessing_drop, axis=1, inplace=True)\n",
    "except KeyError:\n",
    "    # catch to allow executing the cell multiple times\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess 'csa'\n",
    "\n",
    "Make the missing values of the csa column appear at the end\n",
    "by setting them to 'ZZZZZZ'. Then trunkate all entries in\n",
    "by to only the first 3 characters. This reduces the number\n",
    "distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.loc[:,'csa'].isnull(), 'csa'] = 'ZZZZZZ'\n",
    "data['csa'] = data['csa'].map(lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess date columns\n",
    "\n",
    "Convert the variable from a string to a pandas datetime\n",
    "column. Then reduce the number of classes in the\n",
    "categorical variable by limiting the date to year,\n",
    "month and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, 'last_swap'] = pd.to_datetime(data.loc[:, 'last_swap'])\n",
    "data.loc[:, 'last_swap'] = data.loc[:, 'last_swap'].dt.strftime('%Y-%m-%d')\n",
    "# TODO: add time since last swap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Missing values for this variable can be assumed to mean there have been no retention calls made by the customer.\"\n",
    "data.loc[data.loc[:, 'retdays'].isnull(), 'retdays'] = 0\n",
    "# assume avg 3 month minutes for missing values in 6 month minutes\n",
    "data.loc[data.loc[:,'avg6mou'].isnull(), 'avg6mou'] = data.loc[data.loc[:, 'avg6mou'].isnull(), 'avg3mou']\n",
    "data.loc[data.loc[:,'avg6qty'].isnull(), 'avg6qty'] = data.loc[data.loc[:, 'avg6qty'].isnull(), 'avg3qty']\n",
    "data.loc[data.loc[:,'avg6rev'].isnull(), 'avg6rev'] = data.loc[data.loc[:, 'avg6rev'].isnull(), 'avg3rev']\n",
    "# NaNs in age1/age2 should be 0!? (\"00 = default\")\n",
    "data.loc[data.loc[:, 'age1'].isnull(), 'age1'] = 0\n",
    "data.loc[data.loc[:, 'age2'].isnull(), 'age2'] = 0\n",
    "# \"Missing signifies an absence of calls into the retention team.\" !\n",
    "data.loc[data.loc[:, 'tot_ret'].isnull(), 'tot_ret'] = 0\n",
    "# \"Missing signifies an absence of an offer from the retention team.\" -> add '-1' class\n",
    "data.loc[data.loc[:, 'tot_acpt'].isnull(), 'tot_acpt'] = -1\n",
    "# \"A missing value signifies an absence of referrals\" !\n",
    "data.loc[data.loc[:, 'REF_QTY'].isnull(), 'REF_QTY'] = 0\n",
    "# assume no credit card rating change for NaNs\n",
    "data.loc[data.loc[:, 'crtcount'].isnull(), 'crtcount'] = 0\n",
    "# assume no roaming calls, minutes and revenue for NaNs\n",
    "data.loc[data.loc[:, 'rmcalls'].isnull(), 'rmcalls'] = 0\n",
    "data.loc[data.loc[:, 'rmmou'].isnull(), 'rmmou'] = 0\n",
    "data.loc[data.loc[:, 'rmrev'].isnull(), 'rmrev'] = 0\n",
    "# if no current/previous handset price is not known assume 0 ?\n",
    "data.loc[data.loc[:, 'pre_hnd_price'].isnull(), 'pre_hnd_price'] = 0\n",
    "# if number of cars is not known assume 0 / new class\n",
    "data.loc[data.loc[:, 'numbcars'].isnull(), 'numbcars'] = 0\n",
    "# if education is not known create a new class -> 0\n",
    "data.loc[data.loc[:, 'educ1'].isnull(), 'educ1'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in categorical_unordered+categorical_ordered+categorical_binary+preprocessing_date:\n",
    "    # replace float-NaNs with string 'ZZZ' (as to make sure it is ordered last) for python 3.x\n",
    "    data.loc[data.loc[:,column].isnull(),column] = 'ZZZZZ'\n",
    "    data.loc[:,column] = LabelEncoder().fit_transform(data.loc[:,column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest based NA imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "start = datetime.strptime(\"2018-11-12\", \"%Y-%m-%d\").date()\n",
    "\n",
    "end = datetime.strptime(\"2018-12-10\", \"%Y-%m-%d\").date()\n",
    "\n",
    "dates = (start+timedelta(days) for days in range((end-start).days+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Point = namedtuple('Point', 'Index power_001 power_003, power_004 power_006')\n",
    "pt1 = Point(1.0, 5.0, 0, 0, 0)\n",
    "pt2 = Point(2.5, 1.5, 1, 1, 1)\n",
    "stream_ids = [1, 3, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5.0\n",
      "3 0\n",
      "4 0\n",
      "6 0\n"
     ]
    }
   ],
   "source": [
    "for s, v in zip(stream_ids, pt1[1:]):\n",
    "    print(s, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: lor\n",
      "  2: income\n",
      "  3: adults\n",
      "  4: truck\n",
      "  5: rv\n",
      "  6: mtrcycle\n",
      "  7: forgntvl\n",
      "  8: change_rev\n",
      "  9: change_mou\n",
      " 10: hnd_price\n",
      " 11: vceovr_Range\n",
      " 12: vceovr_Mean\n",
      " 13: totmrc_Range\n",
      " 14: totmrc_Mean\n",
      " 15: roam_Range\n",
      " 16: roam_Mean\n",
      " 17: rev_Range\n",
      " 18: rev_Mean\n",
      " 19: ovrrev_Range\n",
      " 20: ovrrev_Mean\n",
      " 21: ovrmou_Range\n",
      " 22: ovrmou_Mean\n",
      " 23: mou_Range\n",
      " 24: mou_Mean\n",
      " 25: datovr_Range\n",
      " 26: datovr_Mean\n",
      " 27: da_Range\n",
      " 28: da_Mean\n",
      " 29: phones\n",
      " 30: models\n",
      " 31: eqpdays\n",
      " DONE\n"
     ]
    }
   ],
   "source": [
    "categorical_imputation = ('lor', 'income', 'adults', 'truck', 'rv', 'mtrcycle','forgntvl', 'phones', 'models')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# get a list of tuples (# missing values, column name)\n",
    "sorted_nans = sorted(\n",
    "    zip(\n",
    "        data.loc[:, data.isnull().sum() > 0].isnull().sum(),\n",
    "        data.loc[:, data.isnull().sum() > 0].columns),\n",
    "    reverse=True)\n",
    "\n",
    "# create a generator that yields the column names excep\n",
    "# for 'churn'. The missing values in the 'churn' variable\n",
    "# are from the test set!\n",
    "sorted_gen = (col for missing, col in sorted_nans if col != 'churn')\n",
    "\n",
    "for column_i, column in enumerate(sorted_gen):\n",
    "    print(f\"{column_i+1:>3}: {column}\")\n",
    "\n",
    "    y = data.loc[:, column]\n",
    "    X = data.loc[:, [col not in (column, 'churn') for col in data.columns]]\n",
    "\n",
    "    row_mask = (y.isnull()==0)\n",
    "    column_mask = X.loc[:,X.isnull().sum()<=0].columns\n",
    "\n",
    "    if column in categorical_imputation:\n",
    "        rfc = RandomForestClassifier(\n",
    "            n_estimators=10, criterion='gini', max_depth=6,\n",
    "            min_samples_split=2, min_samples_leaf=12, \n",
    "            max_features='auto', max_leaf_nodes=None, n_jobs=2)\n",
    "    else:\n",
    "        rfc = RandomForestRegressor(\n",
    "            n_estimators=10, criterion='mse', max_depth=6,\n",
    "            min_samples_split=2, min_samples_leaf=12, \n",
    "            max_features='auto', max_leaf_nodes=None, n_jobs=2)\n",
    "\n",
    "    rfc.fit(X.loc[row_mask, column_mask], y[row_mask])\n",
    "\n",
    "    data.loc[~row_mask, column] = rfc.predict(data.loc[~row_mask, column_mask])\n",
    "\n",
    "print(' DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Encode variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went from 170 to 335 variables\n"
     ]
    }
   ],
   "source": [
    "n_columns_prev = len(data.columns)\n",
    "\n",
    "data = pd.get_dummies(\n",
    "    data=data, dummy_na=False,\n",
    "    columns=categorical_unordered)\n",
    "\n",
    "print(f\"went from {n_columns_prev-1} to {len(data.columns)-1} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Remaining NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "335 / 335 variables left\n"
     ]
    }
   ],
   "source": [
    "# remove columns which still have more than 250 (0.5%) missing values\n",
    "threashold = 250\n",
    "\n",
    "n_columns_prev = len(data.columns)\n",
    "n_missing = data.isnull().sum()\n",
    "\n",
    "print(data.loc[:, (m > threashold and c != 'churn' for m, c in zip(n_missing, data.columns))].columns)\n",
    "data = data.loc[:, (m <= threashold or c == 'churn' for m, c in zip(n_missing, data.columns))]\n",
    "\n",
    "print(f\"{len(data.columns)-1} / {n_columns_prev-1} variables left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point there are no rows with missing values anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.loc[:, (c for c in data.columns if c != 'churn')].isnull().sum(axis=1) > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype uint8, int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint8, int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "StdScaler = StandardScaler(copy=True)\n",
    "StdScaler = StdScaler.fit(data.loc[:, data.columns != 'churn'])\n",
    "data.loc[:, data.columns != 'churn'] = StdScaler.transform(data.loc[:, data.columns != 'churn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset & optimize layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mask = [True if c != 'churn' else False for c in data.columns]\n",
    "y_mask = [True if c == 'churn' else False for c in data.columns]\n",
    "\n",
    "X_tr = data.iloc[:int(len(data)/2), X_mask].values.astype('float32')\n",
    "y_tr = data.iloc[:int(len(data)/2), y_mask].values.astype('float32')\n",
    "\n",
    "X_te = data.iloc[int(len(data)/2):, X_mask].values.astype('float32')\n",
    "y_te = data.iloc[int(len(data)/2):, y_mask].values.astype('float32')\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Cross-Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "data_splits = [(X_tr[tr], X_tr[va], y_tr[tr], y_tr[va]) for tr, va in kf.split(X_tr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifiers\n",
    "#### Define Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcLift(y, y_proba, percentile=0.1):\n",
    "    if len(np.shape(y_proba)) == 2:\n",
    "        y_lift = sorted(zip(y_proba[:,1], y), reverse=True)\n",
    "    else:\n",
    "        y_lift = sorted(zip(y_proba, y), reverse=True)\n",
    "    y_lift = [list(t) for t in zip(*y_lift)][1]\n",
    "    y_lift = y_lift[0:np.int32(len(y_lift)*percentile)]\n",
    "\n",
    "    Pi10 = np.sum(y_lift)/len(y_lift)\n",
    "    PiAll = np.sum(y)/len(y)\n",
    "    Lift10 = Pi10/PiAll\n",
    "    return Lift10\n",
    "\n",
    "def liftScorer(estimator, X, y):\n",
    "    y_proba = None\n",
    "    if hasattr(estimator, 'predict_proba'):\n",
    "        y_proba = estimator.predict_proba(X)\n",
    "    elif hasattr(estimator, 'decision_function'):\n",
    "        y_proba = estimator.decision_function(X)\n",
    "    else:\n",
    "        return 0.0\n",
    "    return calcLift(y, y_proba)\n",
    "\n",
    "def calcLiftCurve(y, y_proba):\n",
    "    PiAll_inv = 1.0/(np.sum(y)/len(y))\n",
    "    \n",
    "    if len(np.shape(y_proba)) == 2:\n",
    "        y_lift = sorted(zip(y_proba[:,1],y),reverse=True)\n",
    "    else:\n",
    "        y_lift = sorted(zip(y_proba,y),reverse=True)\n",
    "    y_lift = [list(t) for t in zip(*y_lift)][1]\n",
    "    \n",
    "    x_axis = np.linspace(0.0, 1.0, 101)\n",
    "    y_axis = np.zeros(np.shape(x_axis), dtype=np.float32)\n",
    "    for i, percentile in enumerate(x_axis):\n",
    "        y_percentile = y_lift[0:np.int32(len(y_lift)*percentile)]\n",
    "        y_axis[i] = (np.sum(y_percentile)/len(y_percentile))*PiAll_inv\n",
    "    return x_axis, y_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Printing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def printLiftMeasureCV(scores):\n",
    "    print(u'{0:2.0f}% Lift Measure:\\t[{3:.3f}, {1:.3f}\\u00B1{2:0.3f}, {4:.3f}]\\n'\n",
    "          .format(0.1*100, np.mean(scores), np.std(scores), np.min(scores), np.max(scores)))\n",
    "    \n",
    "def printClassificationReport(y_pred, y_true, target_names=('NO CHURN', 'CHURN')):\n",
    "    print('Classification Report:\\n\\n', classification_report(y_true, y_pred, target_names=target_names))\n",
    "    \n",
    "def printVariableImportance(feature_importances, column_names):\n",
    "    importance = sorted(zip(feature_importances, column_names), reverse=True)\n",
    "    f1, f2, f3, f4, f5 = importance[0:20], importance[20:40], importance[40:60], importance[60:80], importance[80:100]\n",
    "    print('\\nTop100 Variable Importance:\\n')\n",
    "    for r1,r2,r3,r4,r5 in zip(f1,f2,f3,f4,f5):\n",
    "        print('{0:.2f} {1:17} {2:.2f} {3:17} {4:.2f} {5:17} {6:.2f} {7:17} {8:.2f} {9:17}'\n",
    "              .format(r1[0],r1[1][0:21],r2[0],r2[1][0:21],r3[0],r3[1][0:21],r4[0],r4[1][0:21],r5[0],r5[1][0:21]))\n",
    "        \n",
    "def plot_confusion_matrix(cm, axis, title='Confusion matrix', cmap=plt.cm.Blues, target_names=['NO CHURN', 'CHURN']):\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    for i in [0,1]:\n",
    "        for j in [0,1]:\n",
    "            axis.annotate(str('{0}\\n({1:.2f})'.format(cm[j][i], cm_norm[j][i])), xy=(i-0.1,j+0.06))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save/Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def saveModel(model, model_name):\n",
    "    with open(model_name, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "def loadModel(model_name):\n",
    "    with open(model_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Lift Measure:\t[1.254, 1.308±0.054, 1.361]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:436: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    LogReg = LogisticRegression(\n",
    "        penalty='l2', tol=0.0001,\n",
    "        class_weight=None, random_state=None,\n",
    "        solver='lbfgs', max_iter=100, \n",
    "        dual=False, # n_samples > n_features?False:True\n",
    "        C=100.0)\n",
    "\n",
    "    lift_scores = cross_val_score(\n",
    "        LogReg, X_tr, y=y_tr, scoring=liftScorer, \n",
    "        cv=10, n_jobs=2, verbose=0, pre_dispatch='2*n_jobs')\n",
    "\n",
    "    printLiftMeasureCV(lift_scores)\n",
    "    \n",
    "    LogReg.fit(X_tr, y_tr)\n",
    "\n",
    "    saveModel(LogReg, 'model.LogReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    NO CHURN       0.61      0.62      0.61     25219\n",
      "       CHURN       0.60      0.59      0.60     24781\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     50000\n",
      "   macro avg       0.61      0.61      0.61     50000\n",
      "weighted avg       0.61      0.61      0.61     50000\n",
      "\n",
      "\n",
      "Top100 Variable Importance:\n",
      "\n",
      "1.98 phones            0.14 cc_mou_Range      0.03 mouiwylisv_Range  0.01 unan_dat_Mean     -0.00 age1             \n",
      "1.40 totmrc_Mean       0.14 cc_mou_Mean       0.03 mou_opkd_Mean     0.01 recv_sms_Mean     -0.00 kid16_17         \n",
      "0.93 rev_Range         0.14 blck_dat_Mean     0.03 mtrcycle          0.01 recv_sms_Range    -0.00 mailflag         \n",
      "0.93 avgrev            0.12 adjmou            0.03 solflag           0.01 kid6_10           -0.00 peak_dat_Mean    \n",
      "0.84 recv_vce_Mean     0.12 prizm_social_one  0.02 plcd_vce_Mean     0.01 asl_flag          -0.00 peak_vce_Mean    \n",
      "0.34 roam_Range        0.11 vceovr_Range      0.02 pcowner           0.01 numbcars          -0.01 refurb_new       \n",
      "0.31 callwait_Mean     0.10 mou_rvce_Mean     0.02 attempt_Range     0.01 forgntvl          -0.01 proptype         \n",
      "0.29 adjqty            0.09 da_Range          0.02 area              0.01 drop_vce_Mean     -0.01 actvsubs         \n",
      "0.25 crclscod          0.08 peak_vce_Range    0.02 avg3qty           0.01 drop_blk_Range    -0.01 inonemin_Range   \n",
      "0.25 truck             0.07 attempt_Mean      0.02 opk_dat_Range     0.00 callfwdv_Mean     -0.01 mou_cdat_Range   \n",
      "0.23 new_cell          0.07 crtcount          0.02 rmcalls           0.00 datovr_Range      -0.01 mouiwylisv_Mean  \n",
      "0.21 pre_hnd_price     0.06 ccrndmou_Mean     0.02 totrev            0.00 mailresp          -0.01 unan_vce_Mean    \n",
      "0.21 mou_cvce_Range    0.05 avgqty            0.02 complete_Range    0.00 car_buy           -0.01 rv               \n",
      "0.20 dwlltype          0.04 adjrev            0.02 kid0_2            0.00 mou_opkd_Range    -0.01 kid3_5           \n",
      "0.20 mou_pead_Range    0.04 HHstatin          0.02 comp_vce_Range    0.00 retdays           -0.01 comp_dat_Mean    \n",
      "0.18 iwylis_vce_Mean   0.03 blck_dat_Range    0.01 dwllsize          0.00 income            -0.01 avg3rev          \n",
      "0.18 avgmou            0.03 eqpdays           0.01 age2              0.00 opk_vce_Range     -0.01 drop_vce_Range   \n",
      "0.18 avg6rev           0.03 tot_ret           0.01 mou_peav_Mean     0.00 plcd_vce_Range    -0.01 owylis_vce_Mean  \n",
      "0.15 div_type          0.03 comp_dat_Range    0.01 mou_peav_Range    0.00 change_rev        -0.01 uniqsubs         \n",
      "0.15 owylis_vce_Range  0.03 avg6mou           0.01 drop_blk_Mean     -0.00 ovrrev_Mean       -0.01 infobase         \n"
     ]
    }
   ],
   "source": [
    "LogReg = loadModel('model.LogReg')\n",
    "\n",
    "x_lift_curve, y_lift_curve_lr = calcLiftCurve(y_te, LogReg.predict_proba(X_te))\n",
    "\n",
    "y_pred = LogReg.predict(X_tr)\n",
    "printClassificationReport(y_pred, y_tr)\n",
    "printVariableImportance(LogReg.coef_[0], train_set.loc[:,train_set.columns != 'churn'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Lift Measure:\t[1.088, 1.856±0.327, 2.018]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    GaussNB = GaussianNB()\n",
    "    # CalGaussNB = CalibratedClassifierCV(base_estimator=GaussNB, method='isotonic', cv=4)\n",
    "    \n",
    "    lift_scores = cross_val_score(\n",
    "        GaussNB, X_tr, y=y_tr, scoring=liftScorer, \n",
    "        cv=10, n_jobs=2, verbose=0, pre_dispatch='2*n_jobs')\n",
    "    printLiftMeasureCV(lift_scores)\n",
    "    \n",
    "    GaussNB.fit(X_tr, y_tr)\n",
    "\n",
    "    saveModel(GaussNB, 'model.GaussNB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\RadmerT\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    NO CHURN       0.62      0.07      0.12     25219\n",
      "       CHURN       0.50      0.96      0.66     24781\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     50000\n",
      "   macro avg       0.56      0.51      0.39     50000\n",
      "weighted avg       0.56      0.51      0.39     50000\n",
      "\n",
      "\n",
      "Top100 Variable Importance:\n",
      "\n",
      "0.23 callwait_Mean     0.02 cc_mou_Range      0.01 retdays           -0.01 peak_vce_Range    -0.01 avg6mou          \n",
      "0.14 pre_hnd_price     0.02 solflag           0.01 blck_dat_Mean     -0.01 creditcd          -0.01 blck_vce_Mean    \n",
      "0.12 hnd_price         0.02 blck_vce_Range    0.01 kid6_10           -0.01 owylis_vce_Range  -0.02 kid3_5           \n",
      "0.09 dwlltype          0.02 attempt_Range     0.00 age2              -0.01 avg3qty           -0.02 mailflag         \n",
      "0.07 mailordr          0.02 prizm_social_one  0.00 mou_peav_Range    -0.01 drop_dat_Mean     -0.02 drop_vce_Range   \n",
      "0.07 crclscod          0.02 div_type          0.00 unan_dat_Mean     -0.01 drop_vce_Mean     -0.02 callfwdv_Mean    \n",
      "0.07 avg6qty           0.02 dwllsize          0.00 mou_peav_Mean     -0.01 mtrcycle          -0.02 occu1            \n",
      "0.07 crtcount          0.02 age1              0.00 mailresp          -0.01 ovrmou_Mean       -0.02 change_rev       \n",
      "0.06 marital           0.02 ccrndmou_Mean     0.00 area              -0.01 truck             -0.02 Customer_ID      \n",
      "0.06 new_cell          0.02 tot_ret           -0.00 totrev            -0.01 avg6rev           -0.02 mou_cdat_Range   \n",
      "0.05 comp_vce_Range    0.01 tot_acpt          -0.00 mou_pead_Mean     -0.01 mou_opkd_Mean     -0.02 totcalls         \n",
      "0.05 actvsubs          0.01 numbcars          -0.00 proptype          -0.01 infobase          -0.02 cartype          \n",
      "0.05 recv_sms_Range    0.01 csa               -0.00 unan_vce_Mean     -0.01 adjqty            -0.02 callfwdv_Range   \n",
      "0.05 REF_QTY           0.01 forgntvl          -0.00 kid16_17          -0.01 inonemin_Mean     -0.02 children         \n",
      "0.05 HHstatin          0.01 kid0_2            -0.01 mou_rvce_Mean     -0.01 avg3mou           -0.02 rv               \n",
      "0.04 complete_Mean     0.01 income            -0.01 mou_cvce_Mean     -0.01 educ1             -0.02 rmcalls          \n",
      "0.04 custcare_Range    0.01 rmmou             -0.01 mou_Range         -0.01 opk_vce_Range     -0.02 unan_vce_Range   \n",
      "0.04 mou_Mean          0.01 car_buy           -0.01 callwait_Range    -0.01 mouiwylisv_Range  -0.02 inonemin_Range   \n",
      "0.03 eqpdays           0.01 opk_dat_Mean      -0.01 avgmou            -0.01 rev_Mean          -0.02 mou_rvce_Range   \n",
      "0.03 pcowner           0.01 attempt_Mean      -0.01 threeway_Range    -0.01 ovrmou_Range      -0.02 cc_mou_Mean      \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAFyCAYAAABcAmVDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYhIZJSEREQkMv8PPP0wNsQ2CagAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GaussNB = loadModel('model.GaussNB')\n",
    "\n",
    "x_lift_curve, y_lift_curve_gnb = calcLiftCurve(y_te, GaussNB.predict_proba(X_te))\n",
    "\n",
    "y_pred = GaussNB.predict(X_tr)\n",
    "printClassificationReport(y_pred, y_tr)\n",
    "printVariableImportance(GaussNB.theta_[1]-GaussNB.theta_[0],train_set.loc[:,train_set.columns != 'churn'].columns)\n",
    "print('\\n')\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "ax2 = plt.subplot(1,1,1)\n",
    "plot_confusion_matrix(confusion_matrix(y_tr, y_pred), ax2, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Lift Measure:\t[1.291, 1.367±0.048, 1.441]\n",
      "\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This LinearSVC instance is not fitted yet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-e021af3a515f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprintLiftMeasureCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlift_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mx_lift_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_lift_curve_linsvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalcLiftCurve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mLinSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coef_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             raise NotFittedError(\"This %(name)s instance is not fitted \"\n\u001b[1;32m--> 255\u001b[1;33m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This LinearSVC instance is not fitted yet"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    LinSVM = LinearSVC(\n",
    "        penalty='l2', \n",
    "        loss='squared_hinge', \n",
    "        dual=False, # n_samples > n_features\n",
    "        tol=0.0001, \n",
    "        C=0.1, \n",
    "        fit_intercept=True, \n",
    "        class_weight=None, \n",
    "        max_iter=100)\n",
    "\n",
    "    lift_scores = cross_val_score(\n",
    "        LinSVM, X_tr, y=y_tr, scoring=liftScorer, \n",
    "        cv=10, n_jobs=2, verbose=0, pre_dispatch='2*n_jobs')\n",
    "\n",
    "    printLiftMeasureCV(lift_scores)   \n",
    "\n",
    "    x_lift_curve, y_lift_curve_linsvm = calcLiftCurve(y_tr, LinSVM.decision_function(X_tr))\n",
    "\n",
    "    LinSVM.fit(X_tr, y_tr)\n",
    "\n",
    "    saveModel(LinSVM, 'model.LinSVM') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM = loadModel('model.LinSVM')\n",
    "\n",
    "y_pred = LinSVM.predict(X_tr)\n",
    "printClassificationReport(y_pred, y_tr)\n",
    "printVariableImportance(LinSVM.coef_[0], train_set.loc[:,train_set.columns != 'churn'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # train RandomForest\n",
    "    RandomForest = RandomForestClassifier(\n",
    "        n_estimators=50, \n",
    "        criterion='gini', \n",
    "        max_depth=5,\n",
    "        min_samples_split=2, \n",
    "        min_samples_leaf=1, \n",
    "        max_features='auto', \n",
    "        max_leaf_nodes=None, \n",
    "        n_jobs=1)\n",
    "\n",
    "    lift_scores = cross_val_score(\n",
    "        RandomForest,\n",
    "        X_full, y=y_full,\n",
    "        scoring=liftScorer, cv=10,\n",
    "        n_jobs=4, verbose=0, pre_dispatch='2*n_jobs')\n",
    "    printLiftMeasureCV(lift_scores)\n",
    "\n",
    "    RandomForest.fit(X_tr, y_tr)\n",
    "    \n",
    "    x_lift_curve, y_lift_curve_rf = calcLiftCurve(y_te, RandomForest.predict_proba(X_te))\n",
    "\n",
    "    y_pred = RandomForest.predict(X_te)\n",
    "    printClassificationReport(y_pred, y_te, target_names)\n",
    "    printVariableImportance(RandomForest.feature_importances_, train_set.loc[:,train_set.columns != 'churn'].columns)\n",
    "\n",
    "    saveModel(RandomForest, 'model.RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier\n",
    "I created a script that evaluates the classifier after each boosting\n",
    "iteration and plots the learning curve. This helps to evaluate if the\n",
    "enough boosting iterations are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def gbcEvaluation(estimator, Xtr, ytr, Xva, yva):\n",
    "    '''Evaluates a a GradientBoostingClassifier `estimator`\n",
    "    at every boosting iteration.\n",
    "    \n",
    "    Returns a tuple with\n",
    "    * the `estimator`\n",
    "    * the f1 score for each boosting iteration\n",
    "    * the loss for each boosting iteration\n",
    "    * the lift score for each boosting iteration\n",
    "    * the predictions for each boosting iteration\n",
    "    * the true labels `yva`\n",
    "    '''\n",
    "    estimator.fit(Xtr,ytr)\n",
    "    \n",
    "    iterations = estimator.get_params()['n_estimators']\n",
    "    \n",
    "    # Calculate the predictions and f1-score for each \n",
    "    # boosting iteration.\n",
    "    yva_preds = np.zeros((iterations, len(yva)))\n",
    "    f1 = np.zeros((iterations,), dtype=np.float32)\n",
    "    for l, yva_pred in enumerate(estimator.staged_predict(Xva)):\n",
    "        yva_preds[l,:] = y_pred\n",
    "        f1[l] = f1_score(yva, yva_pred)\n",
    "\n",
    "    # Calculate the loss (e.g. the 'deviance') for each\n",
    "    # boosting iteration.\n",
    "    loss = np.zeros((iterations,), dtype=np.float32)\n",
    "    for i, yva_loss in enumerate(estimator.staged_decision_function(Xva)):\n",
    "        loss[i] = estimator.loss_(yva, yva_loss)\n",
    "\n",
    "    # Calculate the lift score for the 10% of the customers\n",
    "    # from the validaiton set with the highest predicted churn\n",
    "    # score for each boosting iteration.\n",
    "    lift = np.zeros((iterations,), dtype=np.float32)\n",
    "    for j, yva_prob in enumerate(estimator.staged_predict_proba(Xva)):\n",
    "        lift[j] = calcLift(yva, yva_prob)\n",
    "\n",
    "    return (estimator, f1, loss, lift, yva_preds, yva)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gbc = GradientBoostingClassifier(\n",
    "        loss='deviance', \n",
    "        learning_rate=0.1, \n",
    "        n_estimators=200, \n",
    "        min_samples_split=2, \n",
    "        min_samples_leaf=1, \n",
    "        max_depth=6, \n",
    "        subsample=0.5, \n",
    "        max_features=None, \n",
    "        max_leaf_nodes=None, \n",
    "        warm_start=False,\n",
    "        presort='auto',\n",
    "        verbose=0)\n",
    "\n",
    "    iterations = gbc.get_params()['n_estimators']\n",
    "\n",
    "    estimators = []\n",
    "    y_true = []\n",
    "    f1 = np.zeros((iterations,), dtype=np.float32)\n",
    "    loss = np.zeros((iterations,), dtype=np.float32)\n",
    "    lift = np.zeros((iterations,len(data_splits)), dtype=np.float32)\n",
    "    y_predicts = [np.zeros((iterations,len(y_te)), dtype=np.float32) for i in range(0,len(data_splits))]\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_scores = {\n",
    "            executor.submit(gbcEvaluation, clone(gbc), Xtr, ytr, Xva, yva):\n",
    "                (Xtr, Xva, ytr, yva)\n",
    "            for (Xtr, Xva, ytr, yva) in data_splits\n",
    "        }\n",
    "\n",
    "        for k, future in enumerate(as_completed(future_to_scores)):\n",
    "            try:\n",
    "                res_scores = future.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"ERROR:{exc}\")\n",
    "            else:\n",
    "                estimators.append(res_scores[0])\n",
    "                f1 += res_scores[1]\n",
    "                loss += res_scores[2]\n",
    "                lift[:,k] = res_scores[3]\n",
    "                y_predicts[k] = res_scores[4]\n",
    "                y_true.append(res_scores[5])\n",
    "\n",
    "    run_time = time.time()-start_time\n",
    "    print(f\"runtime: {run_time:.1f} s\")\n",
    "    \n",
    "    # average the results\n",
    "    f1 /= len(estimators)\n",
    "    loss /= len(estimators)\n",
    "    y_predicts = np.concatenate(y_predicts, axis=1)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    \n",
    "    # fit individual model for prediction of test dataset\n",
    "    gbc.fit(X_full, y_full)\n",
    "    saveModel([gbc, estimators], 'model.GradientBoosting')\n",
    "    \n",
    "    x_lift_curve, y_lift_curve_gbc = calcLiftCurve(\n",
    "        y_te, gbc.predict_proba(X_te))\n",
    "    \n",
    "    # print average lift scores with min, max, and std\n",
    "    argmax_lift = np.argmax(np.mean(lift,axis=1), axis=0)\n",
    "    print('Score after {0} boosting iterations:\\n'.format(len(lift)))\n",
    "    printLiftMeasureCV(lift[-1])\n",
    "    printClassificationReport(y_predicts[argmax_lift], y_true, target_names)\n",
    "    printVariableImportance(np.mean([x.feature_importances_ for x in estimators],axis=0), train_set.columns[train_set.columns != 'churn'])\n",
    "    print('\\n')\n",
    "    \n",
    "    # plot train and test set deviance after each boosting iteration\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('Deviance')\n",
    "    plt.plot(np.arange(iterations)+1, np.mean([x.train_score_ for x in estimators], axis=0), 'r-', label='deviance on training set')\n",
    "    plt.plot(np.arange(iterations)+1, loss, 'b-', label='deviance on test set')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Boosting Iterations')\n",
    "    plt.ylabel('Deviance')    \n",
    "    # plot F1 measure after each boosting iteration\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('F1 Score')\n",
    "    plt.plot(np.arange(iterations)+1, f1, 'b-', label='F1 score on test set')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Boosting Iterations')\n",
    "    plt.ylabel('F1')\n",
    "    # plot 10% lift measure after every boosting iteration\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('Lift Score')\n",
    "    plt.plot(np.arange(iterations)+1, np.mean(lift, axis=1), 'b-', label='10%-lift on test set')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Boosting Iterations')\n",
    "    plt.ylabel('Lift')\n",
    "    # plot confusion matrix\n",
    "    ax2 = plt.subplot(2,2,4)\n",
    "    plot_confusion_matrix(confusion_matrix(y_true,y_predicts[-1]), ax2, title='Confusion matrix ({0} iter.)'.format(len(lift)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rbfSVM = SVC(C=1.0, \n",
    "                 kernel='rbf', \n",
    "                 probability=False, \n",
    "                 gamma='auto', # 1/n_features\n",
    "                 tol=0.001, \n",
    "                 cache_size=200)\n",
    "    \n",
    "    lift_scores = cross_val_score(rbfSVM, X_full, y=y_full, scoring=liftScorer, \n",
    "                             cv=10, n_jobs=4, verbose=0, pre_dispatch='2*n_jobs')\n",
    "    printLiftMeasureCV(lift_scores)\n",
    "    \n",
    "    rbfSVM.fit(X_tr, y_tr)\n",
    "    \n",
    "    x_lift_curve, y_lift_curve_rbfsvm = calcLiftCurve(y_te, rbfSVM.decision_function(X_te))\n",
    "    \n",
    "    y_pred = rbfSVM.predict(X_te)\n",
    "    printClassificationReport(y_pred, y_te, target_names)\n",
    "    \n",
    "    saveModel(rbfSVM, 'model.rbfSVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Lift Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "# plt.subplot(2,2,1)\n",
    "plt.title('Lift Curves')\n",
    "plt.plot(x_lift_curve, y_lift_curve_gbc, 'r-', label='Gradient Boosting Trees')\n",
    "plt.plot(x_lift_curve, y_lift_curve_rf, 'b-', label='Random Forest')\n",
    "plt.plot(x_lift_curve, y_lift_curve_lr, 'g-', label='Logistic Regression')\n",
    "plt.plot(x_lift_curve, y_lift_curve_linsvm, 'k-', label='Linear SVM')\n",
    "# plt.plot(x_lift_curve, y_lift_curve_rbfsvm, 'c-', label='Gaussian Naive Bayes')\n",
    "plt.plot(x_lift_curve, y_lift_curve_gnb, 'm-', label='Gaussian Naive Bayes')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set.drop('churn',axis=1,inplace=True)\n",
    "# test_set = np.asfortranarray(test_set.values, dtype=np.float32)\n",
    "\n",
    "y_test_proba = gbc.predict_proba(test_set)\n",
    "\n",
    "y_test_proba = pandas.DataFrame(y_test_proba[:,1], index=test_Customer_IDs)\n",
    "y_test_proba.columns = ['EstimatedChurnProbability']\n",
    "\n",
    "y_test_proba.to_csv('BADS_Test_Predictions_GBC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Libraries & Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectKBest\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from scipy.stats import spearmanr\n",
    "import pickle\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def liftScorer(estimator, X, y):\n",
    "    y_proba = None\n",
    "    if hasattr(estimator, 'predict_proba'):\n",
    "        y_proba = estimator.predict_proba(X)\n",
    "    elif hasattr(estimator, 'decision_function'):\n",
    "        y_proba = estimator.decision_function(X)\n",
    "    else:\n",
    "        return 0.0\n",
    "    return calcLift(y, y_proba)\n",
    "\n",
    "def calcLift(y, y_proba, percentile=0.1):\n",
    "    if len(np.shape(y_proba)) == 2:\n",
    "        y_lift = sorted(zip(y_proba[:,1],y),reverse=True)\n",
    "    else:\n",
    "        y_lift = sorted(zip(y_proba,y),reverse=True)\n",
    "    y_lift = [list(t) for t in zip(*y_lift)][1]\n",
    "    y_lift = y_lift[0:np.int32(len(y_lift)*percentile)]\n",
    "\n",
    "    Pi10 = np.sum(y_lift)/len(y_lift)\n",
    "    PiAll = np.sum(y)/len(y)\n",
    "    Lift10 = Pi10/PiAll\n",
    "    return Lift10\n",
    "\n",
    "def calcLiftCurve(y, y_proba):\n",
    "    PiAll_inv = 1.0/(np.sum(y)/len(y))\n",
    "    \n",
    "    if len(np.shape(y_proba)) == 2:\n",
    "        y_lift = sorted(zip(y_proba[:,1],y),reverse=True)\n",
    "    else:\n",
    "        y_lift = sorted(zip(y_proba,y),reverse=True)\n",
    "    y_lift = [list(t) for t in zip(*y_lift)][1]\n",
    "    \n",
    "    x_axis = np.linspace(0.0, 1.0, 101)\n",
    "    y_axis = np.zeros(np.shape(x_axis), dtype=np.float32)\n",
    "    for i, percentile in enumerate(x_axis):\n",
    "        y_percentile = y_lift[0:np.int32(len(y_lift)*percentile)]\n",
    "        y_axis[i] = (np.sum(y_percentile)/len(y_percentile))*PiAll_inv\n",
    "    return x_axis, y_axis\n",
    "\n",
    "def printLiftMeasureCV(scores):\n",
    "    print(u'{0:2.0f}% Lift Measure:\\t[{3:.3f}, {1:.3f}\\u00B1{2:0.3f}, {4:.3f}]\\n'\n",
    "          .format(0.1*100, np.mean(scores), np.std(scores), np.min(scores), np.max(scores)))\n",
    "    \n",
    "def printClassificationReport(y_pred, y_true, target_names):\n",
    "    print('Classification Report:\\n\\n', classification_report(y_true, y_pred, target_names=target_names))\n",
    "    \n",
    "def printVariableImportance(feature_importances, column_names):\n",
    "    importance = sorted(zip(feature_importances, column_names), reverse=True)\n",
    "    f1, f2, f3, f4, f5 = importance[0:20], importance[20:40], importance[40:60], importance[60:80], importance[80:100]\n",
    "    print('\\nTop100 Variable Importance:\\n')\n",
    "    for r1,r2,r3,r4,r5 in zip(f1,f2,f3,f4,f5):\n",
    "        print('{0:.2f} {1:21} {2:.2f} {3:21} {4:.2f} {5:21} {6:.2f} {7:21} {8:.2f} {9:21}'\n",
    "              .format(r1[0],r1[1][0:21],r2[0],r2[1][0:21],r3[0],r3[1][0:21],r4[0],r4[1][0:21],r5[0],r5[1][0:21]))\n",
    "        \n",
    "def plot_confusion_matrix(cm, axis, title='Confusion matrix', cmap=plt.cm.Blues, target_names=['NO CHURN', 'CHURN']):\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm_norm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    for i in [0,1]:\n",
    "        for j in [0,1]:\n",
    "            axis.annotate(str('{0}\\n({1:.2f})'.format(cm[j][i], cm_norm[j][i])), xy=(i-0.1,j+0.06))\n",
    "        \n",
    "def saveModel(model, model_name):\n",
    "    with open(model_name, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "def loadModel(model_name):\n",
    "    with open(model_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "        \n",
    "def generateTrainTestSplit(dataset, test_percent=0.1):\n",
    "    # split the data set into training and test\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(dataset.loc[:,[col for col in dataset.columns if col not in ['churn']]],\n",
    "                                              dataset.loc[:,[col for col in dataset.columns if col in ['churn']]], \n",
    "                                              test_size=test_percent, \n",
    "                                              stratify=dataset.loc[:,[col for col in dataset.columns if col in ['churn']]])\n",
    "\n",
    "    X_tr = np.asfortranarray(X_tr.values, dtype=np.float32)\n",
    "    X_te = np.asfortranarray(X_te.values, dtype=np.float32)\n",
    "    y_tr = np.ravel(np.asfortranarray(y_tr.values, dtype=np.float32))\n",
    "    y_te = np.ravel(np.asfortranarray(y_te.values, dtype=np.float32))\n",
    "    \n",
    "    return X_tr, X_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StdScaler = StandardScaler(copy=True)\n",
    "StdScaler = StdScaler.fit(data.loc[:,data.columns != 'churn'])\n",
    "data.loc[:, data.columns != 'churn'] = StdScaler.transform(data.loc[:,data.columns != 'churn'])\n",
    "\n",
    "train_set = data.iloc[:len(data)/2,:]\n",
    "test_set = data.iloc[len(data)/2:,:]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save or Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_set\n",
    "except NameError:\n",
    "    with open('train.data', 'rb') as f:\n",
    "        train_set = pickle.load(f)\n",
    "else:\n",
    "    if train_set is not None:\n",
    "        with open('train.data', 'wb') as f:\n",
    "            pickle.dump(train_set, f)\n",
    "    else:\n",
    "        with open('train.data', 'rb') as f:\n",
    "            train_set = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    test_set\n",
    "except NameError:\n",
    "    with open('test.data', 'rb') as f:\n",
    "        test_set = pickle.load(f)\n",
    "else:\n",
    "    if test_set is not None:\n",
    "        with open('test.data', 'wb') as f:\n",
    "            pickle.dump(test_set, f)\n",
    "    else:\n",
    "        with open('test.data', 'rb') as f:\n",
    "            test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give classes a name\n",
    "target_names = ('NO CHURN', 'CHURN')\n",
    "\n",
    "data_splits = []\n",
    "\n",
    "for splt in range(0,4):\n",
    "    data_splits.append(generateTrainTestSplit(train_set))\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = generateTrainTestSplit(train_set)\n",
    "\n",
    "# also create a full version for cross-validation\n",
    "X_full = np.asfortranarray(train_set.loc[:,train_set.columns != 'churn'].values, dtype=np.float32)\n",
    "y_full = np.ravel(np.asfortranarray(train_set.loc[:,train_set.columns == 'churn'].values, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce dataset to important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train.data', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "# len(data.columns)\n",
    "\n",
    "# subset_columns = ['csa','eqpdays','ethnic','months','change_mou','mou_Range','adjrev','mou_Mean','totrev','avgrev','churn']\n",
    "# data = data[subset_columns]\n",
    "# len(data.columns)\n",
    "\n",
    "# dummy_mask = [x if (x in data.columns) else None for x in (many_unordered+two_plus_missing)]\n",
    "# dummy_columns = []\n",
    "# for column in dummy_mask:\n",
    "#     if column != None:\n",
    "#         dummy_columns.append(column)\n",
    "# dummy_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     param_grid = dict(max_depth=[3, 9], min_samples_leaf=[1, 50], max_features=[1.0, 0.3])\n",
    "#     grid_search = GridSearchCV(gbc, scoring=scorer, param_grid=param_grid, verbose=3, cv=3, n_jobs=2, pre_dispatch=3)\n",
    "#     grid_search.fit(X_tr,y_tr)\n",
    "#     print(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Gradient Boosting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     gbc = GradientBoostingClassifier(loss='deviance', \n",
    "#                                      learning_rate=0.1, \n",
    "#                                      n_estimators=50, \n",
    "#                                      min_samples_split=2, \n",
    "#                                      min_samples_leaf=1, \n",
    "#                                      max_depth=6, \n",
    "#                                      subsample=0.5, \n",
    "#                                      max_features=None, \n",
    "#                                      max_leaf_nodes=None, \n",
    "#                                      warm_start=False, presort='auto', verbose=0)\n",
    "#     lift_scores = cross_val_score(gbc, X_full, y=y_full, scoring=liftScorer, \n",
    "#                          cv=10, n_jobs=2, verbose=1, pre_dispatch='2*n_jobs')\n",
    "#     printLiftMeasureCV(lift_scores)\n",
    "\n",
    "#     gbc.fit(X_tr, y_tr)\n",
    "#     y_pred = gbc.predict(X_te)\n",
    "#     printClassificationReport(y_pred, y_te, target_names)\n",
    "#     printVariableImportance(gbc.feature_importances_, data.loc[:,data.columns != 'churn'].columns)\n",
    "\n",
    "#     saveModel(gbc, 'GradientBoostingClassifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
